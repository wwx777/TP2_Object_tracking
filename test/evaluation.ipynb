{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83bbf553",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Setup imports and path\n",
    "import sys\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "\n",
    "# make repo importable\n",
    "sys.path.append(str(Path('..').resolve()))\n",
    "from src import evaluation\n",
    "\n",
    "plt.rcParams.update({'figure.max_open_warning': 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ee35c16d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../results/q1_basic', '../results/q6_deep_meanshift']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2) Configure: ground-truth CSV path and results root (adjust as needed)\n",
    "GT_CSV = '../results/gt_mug/gt.csv'  # <-- EDIT THIS to point to your ground-truth CSV\n",
    "RESULTS_ROOT = '../results'  # where your trackers write per-experiment folders\n",
    "CLE_THRESHOLD = 20.0\n",
    "\n",
    "# Collect candidate result folders that contain predictions.csv\n",
    "root = Path(RESULTS_ROOT)\n",
    "candidates = []\n",
    "if root.exists():\n",
    "    for p in sorted(root.iterdir()):\n",
    "        preds = p / 'predictions.csv'\n",
    "        if preds.exists():\n",
    "            candidates.append(str(p))\n",
    "\n",
    "# If no candidates found, you can still manually list directories\n",
    "candidates[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "197033a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>experiment</th>\n",
       "      <th>n_frames</th>\n",
       "      <th>success_rate</th>\n",
       "      <th>precision</th>\n",
       "      <th>mean_iou</th>\n",
       "      <th>mean_cle</th>\n",
       "      <th>fps</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hs_mug</td>\n",
       "      <td>11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.060566</td>\n",
       "      <td>99.112596</td>\n",
       "      <td>13.511501</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  experiment  n_frames  success_rate  precision  mean_iou   mean_cle  \\\n",
       "0     hs_mug        11           0.0        0.0  0.060566  99.112596   \n",
       "\n",
       "         fps  \n",
       "0  13.511501  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3) Run evaluation across candidates and build a DataFrame\n",
    "# Use the specific absolute path provided by the user (override auto-discovery)\n",
    "candidates = [\"/Users/wuwanxing/Documents/projects/python/object_tracking/results/evaluation/hs_mug\"]\n",
    "\n",
    "results = []\n",
    "for pred_dir in candidates:\n",
    "    pred_csv = os.path.join(pred_dir, 'predictions.csv')\n",
    "    try:\n",
    "        res = evaluation.evaluate(pred_csv, GT_CSV, cle_threshold=CLE_THRESHOLD)\n",
    "    except Exception as e:\n",
    "        print(f'Failed to evaluate {pred_dir}: {e}')\n",
    "        continue\n",
    "    res_row = {\n",
    "        'experiment': os.path.basename(pred_dir),\n",
    "        'n_frames': res.get('n_frames'),\n",
    "        'success_rate': res.get('success_rate'),\n",
    "        'precision': res.get('precision'),\n",
    "        'mean_iou': res.get('mean_iou'),\n",
    "        'mean_cle': res.get('mean_cle'),\n",
    "        'fps': res.get('fps')\n",
    "    }\n",
    "    results.append(res_row)\n",
    "\n",
    "# Handle the case where no results were produced to avoid KeyError when sorting/ indexing\n",
    "if not results:\n",
    "    print(\"No evaluation results were produced. Check that `candidates` is not empty and that predictions.csv files exist.\")\n",
    "    df = pd.DataFrame(columns=['experiment','n_frames','success_rate','precision','mean_iou','mean_cle','fps'])\n",
    "else:\n",
    "    df = pd.DataFrame(results)\n",
    "    df = df.sort_values('experiment').reset_index(drop=True)\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
